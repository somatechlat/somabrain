"""Evaluate learning speed: measures precision@1 after incremental seeding.

Two modes:
- Manifest mode (default): reads latest artifacts/benchmarks/seed_manifest_*.json
  generated by an external seeding step and evaluates retrieval quality.
- Auto-seed mode (--autoseed N): synthetically seeds N book+author facts via
  the API, evaluates, and writes a manifest sidecar for reproducibility.

Outputs artifacts/benchmarks/learning_speed_{timestamp}.json and includes
provenance (git sha, python, platform, args) for auditability.
"""

import argparse
import glob
import json
import os
import platform
import subprocess
import sys
from datetime import datetime
from typing import Dict, List

import httpx


def _git_sha() -> str:
    try:
        return (
            subprocess.check_output(
                ["git", "rev-parse", "HEAD"], stderr=subprocess.DEVNULL
            )
            .decode()
            .strip()
        )
    except Exception:
        return "unknown"


def _provenance(args: argparse.Namespace) -> Dict[str, str]:
    return {
        "run_at": datetime.utcnow().strftime("%Y%m%dT%H%M%SZ"),
        "git_sha": _git_sha(),
        "python": sys.version.split(" ")[:1][0],
        "platform": platform.platform(),
        "base_url": args.base,
        "tenant": args.tenant,
        "top_k": args.top_k,
        "checkpoints": args.checkpoints,
        "autoseed": args.autoseed,
    }


def _headers(tenant: str) -> Dict[str, str]:
    return {"Content-Type": "application/json", "X-Tenant-ID": tenant}


def load_manifest() -> Dict:
    paths = sorted(glob.glob("artifacts/benchmarks/seed_manifest_*.json"))
    if not paths:
        raise RuntimeError("No seed manifest found in artifacts/benchmarks/")
    with open(paths[-1], "r", encoding="utf-8") as f:
        return json.load(f)


def autoseed(base: str, tenant: str, count: int) -> Dict:
    """Create a synthetic book/author set via /remember and return a manifest dict."""
    items = [
        {"i": i, "book": f"Book{i}", "author": f"Author{i}"}
        for i in range(1, count + 1)
    ]
    client = httpx.Client(timeout=20.0)
    for it in items:
        fact = f"{it['author']} wrote {it['book']}"
        # Store in fields the retriever/scorer actually uses. The Pydantic payload
        # model ignores unknown keys like 'fact', so include 'content' (and 'task')
        # to ensure the text is embedded and retrievable.
        body = {
            "payload": {
                "task": fact,
                "content": fact,
                "memory_type": "semantic",
                "importance": 1,
            }
        }
        r = client.post(
            base.rstrip("/") + "/memory/remember", json=body, headers=_headers(tenant)
        )
        if r.status_code != 200:
            raise RuntimeError(f"/remember failed: {r.status_code} {r.text}")
    manifest = {
        "base_url": base,
        "tenant": tenant,
        "items": items,
        "created_at": datetime.utcnow().strftime("%Y%m%dT%H%M%SZ"),
        "kind": "autoseed_books",
    }
    os.makedirs("artifacts/benchmarks", exist_ok=True)
    mpath = f"artifacts/benchmarks/seed_manifest_{manifest['created_at']}.json"
    with open(mpath, "w", encoding="utf-8") as f:
        json.dump(manifest, f, indent=2)
    print(f"Wrote seed manifest {mpath}")
    return manifest


def eval_precision_at_1(
    base_url: str, tenant: str, items: List[Dict], top_k: int
) -> Dict[str, int]:
    client = httpx.Client(timeout=20.0)
    url = base_url.rstrip("/") + "/memory/recall"
    correct = 0
    total = 0
    for it in items:
        query = f"Who wrote {it['book']}?"
        body = {"query": query, "top_k": top_k}
        try:
            r = client.post(url, json=body, headers=_headers(tenant))
            total += 1
            if r.status_code == 200:
                j = r.json()
                cands = j.get("results", [])
                # check if top-1 candidate payload contains the author string
                expected = str(it.get("author") or f"Author{it['i']}")
                found = any(expected in str(c.get("payload", {})) for c in cands[:1])
                if found:
                    correct += 1
        except Exception as e:
            print("error", e)
    return {"correct": correct, "total": total}


def main():
    ap = argparse.ArgumentParser(
        description="Evaluate learning speed (precision@1 vs N)"
    )
    from django.conf import settings

    # Use centralized Settings for defaults
    ap.add_argument("--base", default=settings.api_url)
    ap.add_argument("--tenant", default=settings.default_tenant or "learnbench")
    ap.add_argument("--top-k", type=int, default=5)
    ap.add_argument(
        "--checkpoints",
        default="10,50,100,500,1000",
        help="Comma-separated checkpoints to evaluate; 'max' is appended automatically",
    )
    ap.add_argument(
        "--autoseed", type=int, default=0, help="If >0, auto-seed this many items"
    )
    ap.add_argument(
        "--plot",
        action="store_true",
        help="Save a matplotlib learning curve PNG next to the JSON artifact",
    )
    ap.add_argument(
        "--show",
        action="store_true",
        help="If plotting, also display the figure window",
    )
    args = ap.parse_args()

    # Resolve dataset
    if args.autoseed > 0:
        manifest = autoseed(args.base, args.tenant, args.autoseed)
    else:
        manifest = load_manifest()
        # allow overriding base/tenant from CLI even in manifest mode
        manifest["base_url"] = args.base or manifest.get("base_url", args.base)
        manifest["tenant"] = args.tenant or manifest.get("tenant", args.tenant)

    base_url = manifest.get("base_url", args.base)
    tenant = manifest.get("tenant", args.tenant)
    items = manifest.get("items", [])

    # Build checkpoints
    cps: List[int] = []
    for token in str(args.checkpoints).split(","):
        token = token.strip()
        if not token:
            continue
        try:
            cps.append(int(token))
        except ValueError:
            pass
    cps = sorted(set([c for c in cps if c > 0]))
    cps.append(len(items))  # ensure max

    results = []
    for ck in cps:
        subset = items[: min(ck, len(items))]
        stats = eval_precision_at_1(base_url, tenant, subset, args.top_k)
        prec1 = stats["correct"] / max(1, stats["total"])
        results.append({"n": len(subset), "precision_at_1": prec1})
        print(f"n={len(subset)} precision@1={prec1:.3f}")

    os.makedirs("artifacts/benchmarks", exist_ok=True)
    ts = datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
    out = {
        "provenance": _provenance(args),
        "results": results,
    }
    fname = f"artifacts/benchmarks/learning_speed_{ts}.json"
    with open(fname, "w", encoding="utf-8") as f:
        json.dump(out, f, indent=2)
    print(f"Wrote {fname}")

    # Optional matplotlib plot
    if args.plot:
        try:
            import matplotlib.pyplot as plt
        except Exception as e:
            print("matplotlib is required for plotting. Install it to enable --plot.")
            print("Import error:", e)
        else:
            xs = [r["n"] for r in results]
            ys = [r["precision_at_1"] for r in results]
            plt.figure(figsize=(7, 4))
            plt.plot(xs, ys, marker="o")
            plt.xscale("log")
            plt.xlabel("# training items (log scale)")
            plt.ylabel("Precision@1")
            plt.title("Learning curve: Precision@1 vs #items")
            plt.grid(True, alpha=0.3)
            png_path = f"artifacts/benchmarks/learning_curve_{ts}.png"
            plt.savefig(png_path, dpi=150, bbox_inches="tight")
            print(f"Wrote {png_path}")
            if args.show:
                plt.show()


if __name__ == "__main__":
    main()
